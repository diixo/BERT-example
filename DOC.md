
## 1. Архитектура и назначение моделей

### BERT (Bidirectional Encoder Representations from Transformers)

* Это энкодерная модель (encoder-only).

* Обучается на задаче маскированного языкового моделирования (Masked LM): предсказание скрытых слов в тексте.

* Основная цель — получить хорошие контекстные эмбеддинги слов и предложений.

* Обычно используется для понимания текста, классификации, извлечения информации и т.п.


### mBART (Multilingual BART)

* Это энкодер-декодерная модель (seq2seq).

* Основана на BART — вариация трансформера для генерации текста, где декодер обучается восстанавливать текст из зашумленного входа.

* mBART — мультиязычная версия BART, предназначена для задач генерации, например, перевода, суммаризации, переформулирования.


# 2. Контрастное обучение (Contrastive Learning)

* Контрастное обучение — это техника, которая может применяться к любым эмбеддинговым моделям, в том числе к BERT.

* Идея: обучить модель так, чтобы эмбеддинги похожих (семантически близких) предложений были ближе друг к другу в пространстве, а разных — дальше.

* Обычно добавляется специальная loss-функция (например, InfoNCE), чтобы улучшить качество эмбеддингов для задач поиска, кластеризации, семантического сравнения.


# 3. Отличия в применении

* BERT с контрастным обучением — чаще используется для получения мощных эмбеддингов для поиска по смыслу, кластеризации, задачи распознавания парафраз, рекомендации.

* mBART — больше ориентирована на генерацию текста: машинный перевод, исправление ошибок, суммаризация, переформулирование.


### Кратко

* Основное применение BERT с контрастным обучением - Кластеризация, поиск, эмбеддинги

* Основное применение mBART - Перевод, суммаризация, генерация

Если нужна **модель для семантического поиска и сопоставления**, то BERT с контрастным обучением — отличное решение.

Если нужна **модель для генерации или перевода текста на разных языках** — лучше смотреть на mBART.


# BERT

Модель учится "понимать", какие предложения связаны по смыслу, и начинает отображать их как близкие векторные представления (эмбеддинги). Негативные пары — далеко друг от друга, а позитивные — близко.

Это очень полезно для:

* Поиска по смыслу

* Кластеризации текстов

* Построения рекомендательных систем

* Семантического сравнения
